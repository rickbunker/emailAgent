"""
Asset Document E-Mail Ingestion Agent

An intelligent email-driven document processing and filing agent for private market assets.
Handles automatic extraction, classification, and organization of email attachments.

Phase 1 Implementation: ‚úÖ COMPLETE
- Attachment extraction from emails
- SHA256 hashing and duplicate detection  
- ClamAV antivirus integration
- File type validation

Phase 2 Implementation: ‚úÖ COMPLETE
- Qdrant collection setup and management
- Asset definition and storage
- Sender-Asset mapping system
- Contact integration

Phase 3 Implementation: üîÑ IN PROGRESS
- Document classification by type
- AI-powered content analysis  
- Asset identification from email content
- Confidence-based routing decisions
"""

import asyncio
import hashlib
import uuid
import re
from datetime import datetime, timezone
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

try:
    import clamd
except ImportError:
    print("‚ö†Ô∏è ClamAV Python library not installed. Run: pip install clamd")
    clamd = None

try:
    from Levenshtein import distance as levenshtein_distance
except ImportError:
    print("‚ö†Ô∏è Levenshtein library not installed. Run: pip install python-Levenshtein")
    levenshtein_distance = None

try:
    from qdrant_client import QdrantClient
    from qdrant_client.models import (
        CollectionConfig, VectorParams, Distance, 
        PointStruct, Filter, FieldCondition, MatchValue
    )
except ImportError:
    print("‚ö†Ô∏è Qdrant client not installed. Run: pip install qdrant-client")
    QdrantClient = None

# Import email interface
import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

try:
    from email_interface.base import Email, EmailAttachment
except ImportError:
    print("‚ö†Ô∏è Email interface not found. Ensure email_interface module exists.")
    Email = None
    EmailAttachment = None

class ProcessingStatus(Enum):
    """Document processing status"""
    PENDING = "pending"
    PROCESSING = "processing"
    SUCCESS = "success"
    QUARANTINED = "quarantined"
    DUPLICATE = "duplicate"
    INVALID_TYPE = "invalid_type"
    AV_SCAN_FAILED = "av_scan_failed"
    ERROR = "error"

class AssetType(Enum):
    """Private market asset types"""
    COMMERCIAL_REAL_ESTATE = "commercial_real_estate"
    PRIVATE_CREDIT = "private_credit"
    PRIVATE_EQUITY = "private_equity"
    INFRASTRUCTURE = "infrastructure"

class DocumentCategory(Enum):
    """Document classification categories"""
    # Commercial Real Estate
    RENT_ROLL = "rent_roll"
    FINANCIAL_STATEMENTS = "financial_statements"
    PROPERTY_PHOTOS = "property_photos"
    APPRAISAL = "appraisal"
    LEASE_DOCUMENTS = "lease_documents"
    PROPERTY_MANAGEMENT = "property_management"
    
    # Private Credit
    LOAN_DOCUMENTS = "loan_documents"
    BORROWER_FINANCIALS = "borrower_financials"
    COVENANT_COMPLIANCE = "covenant_compliance"
    CREDIT_MEMO = "credit_memo"
    LOAN_MONITORING = "loan_monitoring"
    
    # Private Equity
    PORTFOLIO_REPORTS = "portfolio_reports"
    INVESTOR_UPDATES = "investor_updates"
    BOARD_MATERIALS = "board_materials"
    DEAL_DOCUMENTS = "deal_documents"
    VALUATION_REPORTS = "valuation_reports"
    
    # Infrastructure
    ENGINEERING_REPORTS = "engineering_reports"
    CONSTRUCTION_UPDATES = "construction_updates"
    REGULATORY_DOCUMENTS = "regulatory_documents"
    OPERATIONS_REPORTS = "operations_reports"
    
    # General
    LEGAL_DOCUMENTS = "legal_documents"
    TAX_DOCUMENTS = "tax_documents"
    INSURANCE = "insurance"
    CORRESPONDENCE = "correspondence"
    UNKNOWN = "unknown"

class ConfidenceLevel(Enum):
    """Confidence levels for routing decisions"""
    HIGH = "high"          # ‚â• 90% - Auto-process
    MEDIUM = "medium"      # ‚â• 70% - Process with confirmation
    LOW = "low"           # ‚â• 50% - Save uncategorized  
    VERY_LOW = "very_low" # < 50% - Human review required

@dataclass
class ProcessingResult:
    """Result of document processing"""
    status: ProcessingStatus
    file_hash: Optional[str] = None
    file_path: Optional[Path] = None
    confidence: float = 0.0
    error_message: Optional[str] = None
    quarantine_reason: Optional[str] = None
    duplicate_of: Optional[str] = None
    metadata: Dict[str, Any] = None
    
    # Phase 3: Document Classification
    document_category: Optional[DocumentCategory] = None
    confidence_level: Optional[ConfidenceLevel] = None
    matched_asset_id: Optional[str] = None
    asset_confidence: float = 0.0
    classification_metadata: Dict[str, Any] = None

@dataclass
class AssetDocumentConfig:
    """Configuration for asset document types"""
    allowed_extensions: List[str]
    max_file_size_mb: int
    quarantine_days: int
    version_retention_count: int

@dataclass
class Asset:
    """Asset definition for private market investments"""
    deal_id: str  # UUID
    deal_name: str  # Short name
    asset_name: str  # Full descriptive name
    asset_type: AssetType
    folder_path: str  # File system path
    identifiers: List[str]  # Alternative names/identifiers
    created_date: datetime
    last_updated: datetime
    metadata: Dict[str, Any] = None

@dataclass
class AssetSenderMapping:
    """Many-to-many relationship between assets and senders"""
    mapping_id: str  # UUID
    asset_id: str  # Asset deal_id
    sender_email: str
    confidence: float  # 0.0 to 1.0
    document_types: List[str]  # Expected document categories
    created_date: datetime
    last_activity: datetime
    email_count: int = 0
    metadata: Dict[str, Any] = None

@dataclass
class UnknownSender:
    """Tracking for unknown senders with timeout management"""
    sender_email: str
    first_seen: datetime
    last_seen: datetime
    email_count: int
    pending_documents: List[str]  # File hashes
    timeout_hours: int = 48
    escalated: bool = False
    metadata: Dict[str, Any] = None

class AssetDocumentAgent:
    """
    Asset Document E-Mail Ingestion Agent for processing email attachments
    and organizing them by private market asset classification.
    
    Phase 1: File validation, hashing, and antivirus scanning
    Phase 2: Asset management, sender mapping, and Qdrant integration
    """
    
    # Asset type configurations
    ASSET_CONFIGS = {
        AssetType.COMMERCIAL_REAL_ESTATE: AssetDocumentConfig(
            allowed_extensions=['.pdf', '.xlsx', '.xls', '.jpg', '.png', '.doc', '.docx'],
            max_file_size_mb=50,
            quarantine_days=30,
            version_retention_count=10
        ),
        AssetType.PRIVATE_CREDIT: AssetDocumentConfig(
            allowed_extensions=['.pdf', '.xlsx', '.xls', '.doc', '.docx'],
            max_file_size_mb=25,
            quarantine_days=30,
            version_retention_count=10
        ),
        AssetType.PRIVATE_EQUITY: AssetDocumentConfig(
            allowed_extensions=['.pdf', '.xlsx', '.xls', '.pptx', '.doc', '.docx'],
            max_file_size_mb=100,
            quarantine_days=30,
            version_retention_count=10
        ),
        AssetType.INFRASTRUCTURE: AssetDocumentConfig(
            allowed_extensions=['.pdf', '.xlsx', '.xls', '.dwg', '.jpg', '.png'],
            max_file_size_mb=75,
            quarantine_days=30,
            version_retention_count=10
        )
    }
    
    # Qdrant collection names
    COLLECTIONS = {
        'assets': 'asset_management_assets',
        'asset_sender_mappings': 'asset_management_sender_mappings',
        'processed_documents': 'asset_management_processed_documents',
        'unknown_senders': 'asset_management_unknown_senders'
    }
    
    def __init__(self, 
                 qdrant_client=None,
                 base_assets_path: str = "./assets",
                 clamav_socket: str = None):
        """
        Initialize the Asset Document Agent.
        
        Args:
            qdrant_client: Connected Qdrant client instance
            base_assets_path: Base directory for storing asset documents
            clamav_socket: ClamAV socket path (None for auto-detection)
        """
        self.qdrant = qdrant_client
        self.base_assets_path = Path(base_assets_path)
        self.base_assets_path.mkdir(parents=True, exist_ok=True)
        
        # Initialize ClamAV
        self.clamscan_path = None
        self._init_clamav(clamav_socket)
        
        # Processing statistics
        self.stats = {
            'processed': 0,
            'quarantined': 0,
            'duplicates': 0,
            'errors': 0
        }
        
    def _init_clamav(self, socket_path: str = None):
        """Initialize ClamAV by checking if clamscan is available."""
        import subprocess
        import shutil
        
        # Check if clamscan is available in the system
        clamscan_paths = [
            '/opt/homebrew/bin/clamscan',  # macOS Homebrew
            '/usr/bin/clamscan',           # Linux
            '/usr/local/bin/clamscan',     # Other systems
        ]
        
        self.clamscan_path = None
        
        # First try to find clamscan using which/where
        try:
            result = subprocess.run(['which', 'clamscan'], 
                                  capture_output=True, text=True, timeout=5)
            if result.returncode == 0 and result.stdout.strip():
                self.clamscan_path = result.stdout.strip()
                print(f"‚úÖ Found clamscan at: {self.clamscan_path}")
                return
        except (subprocess.TimeoutExpired, FileNotFoundError):
            pass
        
        # Fallback to checking common paths
        for path in clamscan_paths:
            if Path(path).exists():
                self.clamscan_path = path
                print(f"‚úÖ Found clamscan at: {self.clamscan_path}")
                return
        
        # Try using shutil.which as final fallback
        self.clamscan_path = shutil.which('clamscan')
        if self.clamscan_path:
            print(f"‚úÖ Found clamscan at: {self.clamscan_path}")
        else:
            print("‚ö†Ô∏è ClamAV clamscan not found - antivirus scanning disabled")
            print("   Install ClamAV: brew install clamav (macOS) or apt-get install clamav (Ubuntu)")

    def calculate_file_hash(self, file_content: bytes) -> str:
        """Calculate SHA256 hash of file content."""
        return hashlib.sha256(file_content).hexdigest()
    
    def validate_file_type(self, filename: str, asset_type: AssetType = None) -> bool:
        """
        Validate file type against allowed extensions.
        
        Args:
            filename: Name of the file
            asset_type: Asset type to check against (if None, checks all types)
        """
        file_extension = Path(filename).suffix.lower()
        
        if asset_type:
            config = self.ASSET_CONFIGS[asset_type]
            return file_extension in config.allowed_extensions
        else:
            # Check against all asset types
            all_allowed = set()
            for config in self.ASSET_CONFIGS.values():
                all_allowed.update(config.allowed_extensions)
            return file_extension in all_allowed
    
    def validate_file_size(self, file_size: int, asset_type: AssetType = None) -> bool:
        """
        Validate file size against limits.
        
        Args:
            file_size: Size in bytes
            asset_type: Asset type to check against (if None, uses maximum)
        """
        if asset_type:
            config = self.ASSET_CONFIGS[asset_type]
            max_size = config.max_file_size_mb * 1024 * 1024
        else:
            # Use maximum allowed size across all asset types
            max_size = max(config.max_file_size_mb for config in self.ASSET_CONFIGS.values()) * 1024 * 1024
        
        return file_size <= max_size
    
    async def scan_file_antivirus(self, file_content: bytes, filename: str) -> Tuple[bool, Optional[str]]:
        """
        Scan file content with ClamAV using command-line clamscan.
        
        Returns:
            (is_clean, threat_name)
        """
        if not self.clamscan_path:
            print(f"‚ö†Ô∏è ClamAV not available - skipping scan for {filename}")
            return True, None
        
        import tempfile
        import subprocess
        
        # Save file content to a temporary file
        with tempfile.NamedTemporaryFile(delete=False) as temp_file:
            temp_file.write(file_content)
            temp_path = temp_file.name
        
        try:
            # Run clamscan on the file
            process = subprocess.Popen(
                [self.clamscan_path, '--stdout', '--no-summary', temp_path],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            stdout, stderr = process.communicate(timeout=30)  # 30 second timeout
            
            # Check if virus was found
            if 'Infected files: 1' in stdout or process.returncode == 1:
                # Extract virus name from output
                for line in stdout.splitlines():
                    if temp_path in line and ': ' in line:
                        threat_name = line.split(': ')[1].strip()
                        if threat_name.upper() != 'OK':
                            print(f"ü¶† Virus detected in {filename}: {threat_name}")
                            return False, threat_name
                
                # If we can't extract the specific threat name
                print(f"ü¶† Virus detected in {filename}: Unknown threat")
                return False, "Unknown virus detected"
            
            elif process.returncode == 0:
                # File is clean
                return True, None
            
            else:
                # Some other error occurred
                print(f"‚ö†Ô∏è ClamAV scan error for {filename}: {stderr.strip()}")
                return False, f"Scan error: {stderr.strip()}"
                
        except subprocess.TimeoutExpired:
            print(f"‚ö†Ô∏è ClamAV scan timeout for {filename}")
            return False, "Scan timeout"
        except Exception as e:
            print(f"‚ùå Antivirus scan failed for {filename}: {e}")
            return False, f"Scan error: {str(e)}"
        finally:
            # Clean up the temporary file
            try:
                Path(temp_path).unlink()
            except:
                pass
    
    async def process_single_attachment(self, attachment_data: Dict[str, Any], email_data: Dict[str, Any]) -> ProcessingResult:
        """
        Process a single email attachment through Phase 1 pipeline.
        
        Args:
            attachment_data: Dict with 'filename' and 'content' keys
            email_data: Dict with email metadata
            
        Returns:
            ProcessingResult with status and metadata
        """
        filename = attachment_data.get('filename', 'unknown_attachment')
        content = attachment_data.get('content', b'')
        
        print(f"  üìÑ Processing: {filename}")
        
        try:
            # Step 1: File type validation
            if not self.validate_file_type(filename):
                print(f"    ‚ùå Invalid file type: {Path(filename).suffix}")
                return ProcessingResult(
                    status=ProcessingStatus.INVALID_TYPE,
                    error_message=f"File type {Path(filename).suffix} not allowed"
                )
            
            # Step 2: File size validation
            file_size = len(content)
            if not self.validate_file_size(file_size):
                print(f"    ‚ùå File too large: {file_size / (1024*1024):.1f} MB")
                return ProcessingResult(
                    status=ProcessingStatus.INVALID_TYPE,
                    error_message=f"File size {file_size / (1024*1024):.1f} MB exceeds limit"
                )
            
            # Step 3: Calculate SHA256 hash
            if not content:
                return ProcessingResult(
                    status=ProcessingStatus.ERROR,
                    error_message="No file content available"
                )
            
            file_hash = self.calculate_file_hash(content)
            print(f"    üî¢ SHA256: {file_hash[:16]}...")
            
            # Step 4: Check for duplicates (Phase 2)
            duplicate_id = await self.check_duplicate(file_hash)
            if duplicate_id:
                print(f"    üîÑ Duplicate detected (original: {duplicate_id})")
                return ProcessingResult(
                    status=ProcessingStatus.DUPLICATE,
                    file_hash=file_hash,
                    duplicate_of=duplicate_id
                )
            
            # Step 5: Antivirus scan
            is_clean, threat_name = await self.scan_file_antivirus(content, filename)
            if not is_clean:
                print(f"    ü¶† Quarantined: {threat_name}")
                
                return ProcessingResult(
                    status=ProcessingStatus.QUARANTINED,
                    file_hash=file_hash,
                    quarantine_reason=threat_name
                )
            
            print(f"    ‚úÖ File passed all Phase 1 validation checks")
            
            # Prepare metadata for future phases
            metadata = {
                'filename': filename,
                'file_size': file_size,
                'sender_email': email_data.get('sender_email'),
                'sender_name': email_data.get('sender_name'),
                'email_subject': email_data.get('subject'),
                'email_date': email_data.get('date'),
                'processing_date': datetime.now(timezone.utc).isoformat(),
                'file_extension': Path(filename).suffix.lower(),
                'validation_passed': True
            }
            
            return ProcessingResult(
                status=ProcessingStatus.SUCCESS,
                file_hash=file_hash,
                confidence=1.0,  # Full confidence for validation
                metadata=metadata
            )
            
        except Exception as e:
            print(f"    ‚ùå Processing error: {e}")
            return ProcessingResult(
                status=ProcessingStatus.ERROR,
                error_message=str(e)
            )
    
    def get_processing_stats(self) -> Dict[str, Any]:
        """Get current processing statistics."""
        total = sum(self.stats.values())
        
        return {
            'total_processed': total,
            'successful': self.stats['processed'],
            'quarantined': self.stats['quarantined'],
            'duplicates': self.stats['duplicates'],
            'errors': self.stats['errors'],
            'success_rate': (self.stats['processed'] / total * 100) if total > 0 else 0.0
        }
    
    async def health_check(self) -> Dict[str, Any]:
        """Perform agent health check."""
        health = {
            'clamscan_path': self.clamscan_path is not None,
            'base_path_writable': False,
            'asset_configs_loaded': True
        }
        
        try:
            # Check file system
            test_file = self.base_assets_path / '.health_check'
            test_file.touch()
            test_file.unlink()
            health['base_path_writable'] = True
        except:
            pass
        
        return health
    
    # ================================
    # PHASE 2: Asset & Sender Management
    # ================================
    
    async def initialize_collections(self):
        """Initialize Qdrant collections for asset management."""
        if not self.qdrant:
            print("‚ö†Ô∏è Qdrant client not provided - skipping collection initialization")
            return False
            
        try:
            # Assets collection with vector embeddings for semantic search
            if not await self._collection_exists(self.COLLECTIONS['assets']):
                await self._create_collection(
                    self.COLLECTIONS['assets'],
                    vector_size=384,  # sentence-transformers dimension
                    description="Asset definitions with semantic embeddings"
                )
                print(f"‚úÖ Created assets collection: {self.COLLECTIONS['assets']}")
            
            # Asset-Sender mappings (no vectors needed)
            if not await self._collection_exists(self.COLLECTIONS['asset_sender_mappings']):
                await self._create_collection(
                    self.COLLECTIONS['asset_sender_mappings'],
                    vector_size=1,  # Minimal vector for Qdrant requirement
                    description="Many-to-many asset-sender relationships"
                )
                print(f"‚úÖ Created asset-sender mappings collection")
            
            # Processed documents (no vectors needed)
            if not await self._collection_exists(self.COLLECTIONS['processed_documents']):
                await self._create_collection(
                    self.COLLECTIONS['processed_documents'],
                    vector_size=1,  # Minimal vector
                    description="Processed document metadata and file hashes"
                )
                print(f"‚úÖ Created processed documents collection")
            
            # Unknown senders (no vectors needed)
            if not await self._collection_exists(self.COLLECTIONS['unknown_senders']):
                await self._create_collection(
                    self.COLLECTIONS['unknown_senders'],
                    vector_size=1,  # Minimal vector
                    description="Unknown sender tracking and timeout management"
                )
                print(f"‚úÖ Created unknown senders collection")
            
            print("üéØ All Qdrant collections initialized successfully")
            return True
            
        except Exception as e:
            print(f"‚ùå Failed to initialize Qdrant collections: {e}")
            return False
    
    async def _collection_exists(self, collection_name: str) -> bool:
        """Check if a Qdrant collection exists."""
        try:
            collections = self.qdrant.get_collections()
            return any(c.name == collection_name for c in collections.collections)
        except:
            return False
    
    async def _create_collection(self, collection_name: str, vector_size: int, description: str):
        """Create a Qdrant collection."""
        self.qdrant.create_collection(
            collection_name=collection_name,
            vectors_config=VectorParams(
                size=vector_size,
                distance=Distance.COSINE
            )
        )
    
    async def create_asset(self, 
                          deal_name: str,
                          asset_name: str,
                          asset_type: AssetType,
                          identifiers: List[str] = None) -> str:
        """
        Create a new asset definition.
        
        Args:
            deal_name: Short name for the asset
            asset_name: Full descriptive name
            asset_type: Type of private market asset
            identifiers: Alternative names/identifiers
            
        Returns:
            Asset deal_id (UUID)
        """
        deal_id = str(uuid.uuid4())
        folder_path = f"./assets/{deal_id}_{deal_name.replace(' ', '_')}"
        
        asset = Asset(
            deal_id=deal_id,
            deal_name=deal_name,
            asset_name=asset_name,
            asset_type=asset_type,
            folder_path=folder_path,
            identifiers=identifiers or [],
            created_date=datetime.now(timezone.utc),
            last_updated=datetime.now(timezone.utc)
        )
        
        if self.qdrant:
            try:
                # Store asset in Qdrant (with dummy vector for now)
                dummy_vector = [0.0] * 384  # Will be replaced with semantic embeddings
                
                point = PointStruct(
                    id=deal_id,
                    vector=dummy_vector,
                    payload={
                        'deal_id': deal_id,
                        'deal_name': deal_name,
                        'asset_name': asset_name,
                        'asset_type': asset_type.value,
                        'folder_path': folder_path,
                        'identifiers': identifiers or [],
                        'created_date': asset.created_date.isoformat(),
                        'last_updated': asset.last_updated.isoformat()
                    }
                )
                
                self.qdrant.upsert(
                    collection_name=self.COLLECTIONS['assets'],
                    points=[point]
                )
                
                print(f"‚úÖ Created asset: {deal_name} ({deal_id})")
                
            except Exception as e:
                print(f"‚ùå Failed to store asset in Qdrant: {e}")
                
        # Create folder structure
        Path(folder_path).mkdir(parents=True, exist_ok=True)
        
        return deal_id
    
    async def get_asset(self, deal_id: str) -> Optional[Asset]:
        """Retrieve asset by deal_id."""
        if not self.qdrant:
            return None
            
        try:
            search_result = self.qdrant.scroll(
                collection_name=self.COLLECTIONS['assets'],
                scroll_filter=Filter(
                    must=[
                        FieldCondition(
                            key="deal_id",
                            match=MatchValue(value=deal_id)
                        )
                    ]
                ),
                limit=1
            )
            
            if search_result[0]:
                payload = search_result[0][0].payload
                return Asset(
                    deal_id=payload['deal_id'],
                    deal_name=payload['deal_name'],
                    asset_name=payload['asset_name'],
                    asset_type=AssetType(payload['asset_type']),
                    folder_path=payload['folder_path'],
                    identifiers=payload['identifiers'],
                    created_date=datetime.fromisoformat(payload['created_date']),
                    last_updated=datetime.fromisoformat(payload['last_updated'])
                )
                
            return None
            
        except Exception as e:
            print(f"‚ùå Failed to retrieve asset: {e}")
            return None
    
    async def list_assets(self) -> List[Asset]:
        """List all assets."""
        assets = []
        
        if not self.qdrant:
            return assets
            
        try:
            search_result = self.qdrant.scroll(
                collection_name=self.COLLECTIONS['assets'],
                limit=100  # Adjust as needed
            )
            
            for point in search_result[0]:
                payload = point.payload
                asset = Asset(
                    deal_id=payload['deal_id'],
                    deal_name=payload['deal_name'],
                    asset_name=payload['asset_name'],
                    asset_type=AssetType(payload['asset_type']),
                    folder_path=payload['folder_path'],
                    identifiers=payload['identifiers'],
                    created_date=datetime.fromisoformat(payload['created_date']),
                    last_updated=datetime.fromisoformat(payload['last_updated'])
                )
                assets.append(asset)
                
        except Exception as e:
            print(f"‚ùå Failed to list assets: {e}")
            
        return assets
    
    async def create_asset_sender_mapping(self,
                                        asset_id: str,
                                        sender_email: str,
                                        confidence: float,
                                        document_types: List[str] = None) -> str:
        """
        Create asset-sender mapping.
        
        Args:
            asset_id: Asset deal_id
            sender_email: Email address of sender
            confidence: Confidence level (0.0 to 1.0)
            document_types: Expected document categories
            
        Returns:
            Mapping ID (UUID)
        """
        mapping_id = str(uuid.uuid4())
        
        mapping = AssetSenderMapping(
            mapping_id=mapping_id,
            asset_id=asset_id,
            sender_email=sender_email.lower(),
            confidence=confidence,
            document_types=document_types or [],
            created_date=datetime.now(timezone.utc),
            last_activity=datetime.now(timezone.utc)
        )
        
        if self.qdrant:
            try:
                # Store mapping in Qdrant
                dummy_vector = [0.0]  # Minimal vector
                
                point = PointStruct(
                    id=mapping_id,
                    vector=dummy_vector,
                    payload={
                        'mapping_id': mapping_id,
                        'asset_id': asset_id,
                        'sender_email': sender_email.lower(),
                        'confidence': confidence,
                        'document_types': document_types or [],
                        'created_date': mapping.created_date.isoformat(),
                        'last_activity': mapping.last_activity.isoformat(),
                        'email_count': 0
                    }
                )
                
                self.qdrant.upsert(
                    collection_name=self.COLLECTIONS['asset_sender_mappings'],
                    points=[point]
                )
                
                print(f"‚úÖ Created asset-sender mapping: {sender_email} ‚Üí {asset_id}")
                
            except Exception as e:
                print(f"‚ùå Failed to store mapping in Qdrant: {e}")
                
        return mapping_id
    
    async def get_sender_assets(self, sender_email: str) -> List[Dict[str, Any]]:
        """Get all assets associated with a sender."""
        assets = []
        
        if not self.qdrant:
            return assets
            
        try:
            search_result = self.qdrant.scroll(
                collection_name=self.COLLECTIONS['asset_sender_mappings'],
                scroll_filter=Filter(
                    must=[
                        FieldCondition(
                            key="sender_email",
                            match=MatchValue(value=sender_email.lower())
                        )
                    ]
                ),
                limit=50
            )
            
            for point in search_result[0]:
                payload = point.payload
                assets.append({
                    'mapping_id': payload['mapping_id'],
                    'asset_id': payload['asset_id'],
                    'confidence': payload['confidence'],
                    'document_types': payload['document_types'],
                    'email_count': payload.get('email_count', 0)
                })
                
        except Exception as e:
            print(f"‚ùå Failed to get sender assets: {e}")
            
        return assets
    
    async def check_duplicate(self, file_hash: str) -> Optional[str]:
        """
        Check if file hash already exists in processed documents.
        
        Returns:
            Document ID if duplicate found, None otherwise
        """
        if not self.qdrant:
            return None
            
        try:
            search_result = self.qdrant.scroll(
                collection_name=self.COLLECTIONS['processed_documents'],
                scroll_filter=Filter(
                    must=[
                        FieldCondition(
                            key="file_hash",
                            match=MatchValue(value=file_hash)
                        )
                    ]
                ),
                limit=1
            )
            
            if search_result[0]:  # Found duplicate
                return search_result[0][0].payload.get('document_id')
            
            return None
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error checking for duplicates: {e}")
            return None
    
    async def store_processed_document(self, 
                                     file_hash: str,
                                     processing_result: ProcessingResult,
                                     asset_id: str = None) -> str:
        """Store processed document metadata."""
        document_id = str(uuid.uuid4())
        
        if not self.qdrant:
            return document_id
            
        try:
            dummy_vector = [0.0]  # Minimal vector
            
            payload = {
                'document_id': document_id,
                'file_hash': file_hash,
                'status': processing_result.status.value,
                'confidence': processing_result.confidence,
                'processing_date': datetime.now(timezone.utc).isoformat(),
                'metadata': processing_result.metadata or {}
            }
            
            if asset_id:
                payload['asset_id'] = asset_id
                
            if processing_result.file_path:
                payload['file_path'] = str(processing_result.file_path)
                
            point = PointStruct(
                id=document_id,
                vector=dummy_vector,
                payload=payload
            )
            
            self.qdrant.upsert(
                collection_name=self.COLLECTIONS['processed_documents'],
                points=[point]
            )
            
        except Exception as e:
            print(f"‚ùå Failed to store processed document: {e}")
            
        return document_id
    
    # ================================
    # PHASE 3: Document Classification
    # ================================
    
    # Document classification patterns by asset type
    CLASSIFICATION_PATTERNS = {
        AssetType.COMMERCIAL_REAL_ESTATE: {
            DocumentCategory.RENT_ROLL: [
                r'rent.*roll', r'rental.*income', r'tenant.*list', r'occupancy.*report',
                r'lease.*schedule', r'rental.*schedule'
            ],
            DocumentCategory.FINANCIAL_STATEMENTS: [
                r'financial.*statement', r'income.*statement', r'balance.*sheet',
                r'cash.*flow', r'profit.*loss', r'p.*l.*statement'
            ],
            DocumentCategory.PROPERTY_PHOTOS: [
                r'photo', r'image', r'picture', r'exterior', r'interior', r'amenity'
            ],
            DocumentCategory.APPRAISAL: [
                r'appraisal', r'valuation', r'market.*value', r'property.*value'
            ],
            DocumentCategory.LEASE_DOCUMENTS: [
                r'lease.*agreement', r'lease.*contract', r'tenancy.*agreement',
                r'rental.*agreement', r'lease.*amendment'
            ],
            DocumentCategory.PROPERTY_MANAGEMENT: [
                r'management.*report', r'maintenance.*report', r'inspection.*report',
                r'property.*condition', r'capex', r'capital.*expenditure'
            ]
        },
        AssetType.PRIVATE_CREDIT: {
            DocumentCategory.LOAN_DOCUMENTS: [
                r'loan.*agreement', r'credit.*agreement', r'facility.*agreement',
                r'promissory.*note', r'security.*agreement'
            ],
            DocumentCategory.BORROWER_FINANCIALS: [
                r'borrower.*financial', r'financial.*statement', r'credit.*memo',
                r'financial.*performance', r'quarterly.*report'
            ],
            DocumentCategory.COVENANT_COMPLIANCE: [
                r'covenant.*compliance', r'compliance.*certificate', r'covenant.*test',
                r'financial.*covenant', r'compliance.*report'
            ],
            DocumentCategory.CREDIT_MEMO: [
                r'credit.*memo', r'investment.*memo', r'credit.*analysis',
                r'risk.*assessment', r'underwriting.*memo'
            ],
            DocumentCategory.LOAN_MONITORING: [
                r'monitoring.*report', r'portfolio.*report', r'loan.*performance',
                r'payment.*history', r'default.*report'
            ]
        },
        AssetType.PRIVATE_EQUITY: {
            DocumentCategory.PORTFOLIO_REPORTS: [
                r'portfolio.*report', r'portfolio.*update', r'company.*update',
                r'performance.*report', r'investment.*update'
            ],
            DocumentCategory.INVESTOR_UPDATES: [
                r'investor.*update', r'investor.*letter', r'quarterly.*update',
                r'annual.*report', r'fund.*update'
            ],
            DocumentCategory.BOARD_MATERIALS: [
                r'board.*material', r'board.*deck', r'board.*presentation',
                r'board.*meeting', r'director.*report'
            ],
            DocumentCategory.DEAL_DOCUMENTS: [
                r'purchase.*agreement', r'merger.*agreement', r'acquisition',
                r'transaction.*document', r'closing.*document'
            ],
            DocumentCategory.VALUATION_REPORTS: [
                r'valuation.*report', r'valuation.*analysis', r'fair.*value',
                r'mark.*to.*market', r'portfolio.*valuation'
            ]
        },
        AssetType.INFRASTRUCTURE: {
            DocumentCategory.ENGINEERING_REPORTS: [
                r'engineering.*report', r'technical.*report', r'design.*document',
                r'structural.*report', r'environmental.*study'
            ],
            DocumentCategory.CONSTRUCTION_UPDATES: [
                r'construction.*update', r'progress.*report', r'milestone.*report',
                r'construction.*status', r'build.*progress'
            ],
            DocumentCategory.REGULATORY_DOCUMENTS: [
                r'permit', r'license', r'regulatory.*approval', r'compliance.*document',
                r'environmental.*clearance', r'zoning.*approval'
            ],
            DocumentCategory.OPERATIONS_REPORTS: [
                r'operations.*report', r'performance.*metrics', r'utilization.*report',
                r'maintenance.*log', r'operations.*update'
            ]
        }
    }
    
    # Asset name patterns for fuzzy matching
    ASSET_KEYWORDS = {
        'commercial_real_estate': [
            'property', 'building', 'office', 'retail', 'warehouse', 'industrial',
            'commercial', 'plaza', 'center', 'tower', 'complex', 'development'
        ],
        'private_credit': [
            'loan', 'credit', 'facility', 'debt', 'financing', 'bridge',
            'term', 'revolving', 'senior', 'subordinate', 'mezzanine'
        ],
        'private_equity': [
            'equity', 'investment', 'portfolio', 'fund', 'acquisition',
            'buyout', 'growth', 'venture', 'capital', 'holdings'
        ],
        'infrastructure': [
            'infrastructure', 'utility', 'energy', 'transportation', 'telecom',
            'power', 'water', 'gas', 'pipeline', 'renewable', 'solar', 'wind'
        ]
    }
    
    def classify_document(self, 
                         filename: str, 
                         email_subject: str,
                         email_body: str = "",
                         asset_type: AssetType = None) -> Tuple[DocumentCategory, float]:
        """
        Classify document based on filename, email subject, and content.
        
        Args:
            filename: Name of the attachment
            email_subject: Email subject line
            email_body: Email body content
            asset_type: Known asset type (if available)
            
        Returns:
            (DocumentCategory, confidence_score)
        """
        # Combine all text for analysis
        combined_text = f"{filename} {email_subject} {email_body}".lower()
        
        # If asset type is known, only check patterns for that type
        if asset_type and asset_type in self.CLASSIFICATION_PATTERNS:
            patterns_to_check = {asset_type: self.CLASSIFICATION_PATTERNS[asset_type]}
        else:
            patterns_to_check = self.CLASSIFICATION_PATTERNS
        
        best_category = DocumentCategory.UNKNOWN
        best_score = 0.0
        
        for asset_type_key, categories in patterns_to_check.items():
            for category, patterns in categories.items():
                score = 0.0
                matches = 0
                
                for pattern in patterns:
                    if re.search(pattern, combined_text):
                        matches += 1
                        # Weight filename matches higher than subject/body
                        if re.search(pattern, filename.lower()):
                            score += 0.6
                        elif re.search(pattern, email_subject.lower()):
                            score += 0.3
                        else:
                            score += 0.1
                
                # Normalize score by number of patterns
                if patterns:
                    normalized_score = min(score / len(patterns), 1.0)
                    
                    # Boost score if multiple patterns match
                    if matches > 1:
                        normalized_score = min(normalized_score * 1.2, 1.0)
                    
                    if normalized_score > best_score:
                        best_score = normalized_score
                        best_category = category
        
        return best_category, best_score
    
    def identify_asset_from_content(self, 
                                   email_subject: str,
                                   email_body: str = "",
                                   filename: str = "",
                                   known_assets: List[Asset] = None) -> List[Tuple[str, float]]:
        """
        Identify potential assets mentioned in email content using fuzzy matching.
        
        Args:
            email_subject: Email subject line
            email_body: Email body content  
            filename: Attachment filename
            known_assets: List of known assets to match against
            
        Returns:
            List of (asset_id, confidence_score) tuples, sorted by confidence
        """
        if not known_assets or not levenshtein_distance:
            return []
        
        # Combine text for analysis
        combined_text = f"{email_subject} {email_body} {filename}".lower()
        
        asset_matches = []
        
        for asset in known_assets:
            max_confidence = 0.0
            
            # Check all asset identifiers
            all_identifiers = [asset.deal_name, asset.asset_name] + asset.identifiers
            
            for identifier in all_identifiers:
                if not identifier:
                    continue
                    
                identifier_lower = identifier.lower()
                
                # Exact match (highest confidence)
                if identifier_lower in combined_text:
                    confidence = 0.95
                    max_confidence = max(max_confidence, confidence)
                    continue
                
                # Fuzzy matching with Levenshtein distance
                words = combined_text.split()
                for word_group_size in [3, 2, 1]:  # Check phrases of different lengths
                    for i in range(len(words) - word_group_size + 1):
                        phrase = " ".join(words[i:i + word_group_size])
                        
                        if len(phrase) < 3:  # Skip very short phrases
                            continue
                            
                        # Calculate similarity
                        distance = levenshtein_distance(identifier_lower, phrase)
                        max_len = max(len(identifier_lower), len(phrase))
                        
                        if max_len > 0:
                            similarity = 1 - (distance / max_len)
                            
                            # Threshold for fuzzy matches
                            if similarity >= 0.8:
                                confidence = similarity * 0.9  # Slightly lower than exact match
                                max_confidence = max(max_confidence, confidence)
            
            # Add partial word matching for specific keywords
            asset_type_keywords = self.ASSET_KEYWORDS.get(asset.asset_type.value, [])
            keyword_matches = sum(1 for keyword in asset_type_keywords if keyword in combined_text)
            
            if keyword_matches > 0:
                keyword_boost = min(keyword_matches * 0.1, 0.3)
                max_confidence = min(max_confidence + keyword_boost, 1.0)
            
            if max_confidence > 0.5:  # Only include reasonable matches
                asset_matches.append((asset.deal_id, max_confidence))
        
        # Sort by confidence (highest first)
        asset_matches.sort(key=lambda x: x[1], reverse=True)
        
        return asset_matches
    
    def determine_confidence_level(self, 
                                  document_confidence: float,
                                  asset_confidence: float,
                                  sender_known: bool) -> ConfidenceLevel:
        """
        Determine overall confidence level for routing decisions.
        
        Args:
            document_confidence: Confidence in document classification
            asset_confidence: Confidence in asset identification
            sender_known: Whether sender is in asset-sender mappings
            
        Returns:
            ConfidenceLevel for routing decision
        """
        # Calculate weighted confidence
        weights = {
            'document': 0.4,
            'asset': 0.4,
            'sender': 0.2
        }
        
        sender_score = 1.0 if sender_known else 0.0
        
        overall_confidence = (
            document_confidence * weights['document'] +
            asset_confidence * weights['asset'] +
            sender_score * weights['sender']
        )
        
        # Map to confidence levels with thresholds from requirements
        if overall_confidence >= 0.90:
            return ConfidenceLevel.HIGH      # Auto-process
        elif overall_confidence >= 0.70:
            return ConfidenceLevel.MEDIUM    # Process with confirmation
        elif overall_confidence >= 0.50:
            return ConfidenceLevel.LOW       # Save uncategorized
        else:
            return ConfidenceLevel.VERY_LOW  # Human review required
    
    async def enhanced_process_attachment(self, 
                                        attachment_data: Dict[str, Any], 
                                        email_data: Dict[str, Any]) -> ProcessingResult:
        """
        Enhanced attachment processing with Phase 3 classification and intelligence.
        
        Args:
            attachment_data: Dict with 'filename' and 'content' keys
            email_data: Dict with email metadata
            
        Returns:
            ProcessingResult with classification and asset matching
        """
        # Start with Phase 1 & 2 processing
        result = await self.process_single_attachment(attachment_data, email_data)
        
        if result.status != ProcessingStatus.SUCCESS:
            return result
        
        # Phase 3: Add document classification and asset identification
        filename = attachment_data.get('filename', '')
        email_subject = email_data.get('subject', '')
        email_body = email_data.get('body', '')
        sender_email = email_data.get('sender_email', '')
        
        print(f"    üß† Phase 3: Analyzing content and identifying assets...")
        
        # Step 1: Document Classification
        document_category, doc_confidence = self.classify_document(
            filename, email_subject, email_body
        )
        
        print(f"    üìã Document classified as: {document_category.value} (confidence: {doc_confidence:.2f})")
        
        # Step 2: Asset Identification
        known_assets = await self.list_assets()
        asset_matches = self.identify_asset_from_content(
            email_subject, email_body, filename, known_assets
        )
        
        matched_asset_id = None
        asset_confidence = 0.0
        
        if asset_matches:
            matched_asset_id, asset_confidence = asset_matches[0]  # Best match
            asset = await self.get_asset(matched_asset_id)
            asset_name = asset.deal_name if asset else matched_asset_id[:8]
            print(f"    üéØ Best asset match: {asset_name} (confidence: {asset_confidence:.2f})")
        else:
            print(f"    ‚ùì No asset matches found")
        
        # Step 3: Check if sender is known
        sender_assets = await self.get_sender_assets(sender_email)
        sender_known = len(sender_assets) > 0
        
        if sender_known:
            print(f"    üë§ Known sender: {len(sender_assets)} asset(s) associated")
            # If sender has assets but we didn't match content, use sender's highest confidence asset
            if not matched_asset_id and sender_assets:
                best_sender_asset = max(sender_assets, key=lambda x: x['confidence'])
                matched_asset_id = best_sender_asset['asset_id']
                asset_confidence = best_sender_asset['confidence'] * 0.8  # Slightly reduce confidence
                print(f"    üîó Using sender's primary asset: {matched_asset_id[:8]}...")
        else:
            print(f"    ‚ùì Unknown sender: {sender_email}")
        
        # Step 4: Determine overall confidence level
        confidence_level = self.determine_confidence_level(
            doc_confidence, asset_confidence, sender_known
        )
        
        confidence_icons = {
            ConfidenceLevel.HIGH: "üü¢",
            ConfidenceLevel.MEDIUM: "üü°", 
            ConfidenceLevel.LOW: "üü†",
            ConfidenceLevel.VERY_LOW: "üî¥"
        }
        
        icon = confidence_icons[confidence_level]
        print(f"    {icon} Overall confidence: {confidence_level.value}")
        
        # Step 5: Update result with Phase 3 data
        result.document_category = document_category
        result.confidence_level = confidence_level
        result.matched_asset_id = matched_asset_id
        result.asset_confidence = asset_confidence
        result.classification_metadata = {
            'document_confidence': doc_confidence,
            'asset_confidence': asset_confidence,
            'sender_known': sender_known,
            'asset_matches': asset_matches[:3],  # Top 3 matches
            'sender_asset_count': len(sender_assets)
        }
        
        return result

# Example usage and testing
async def test_asset_document_agent():
    """Test function for the Asset Document Agent Phase 1 implementation."""
    print("üß™ Testing Asset Document Agent - Phase 1")
    print("=" * 60)
    
    # Initialize agent without Qdrant for basic testing
    agent = AssetDocumentAgent()
    
    # Test health check
    health = await agent.health_check()
    print(f"üè• Agent health check: {health}")
    
    # Test file validation
    print(f"\nüìã Testing file validation:")
    print(f"   PDF file: {agent.validate_file_type('document.pdf')}")
    print(f"   Excel file: {agent.validate_file_type('spreadsheet.xlsx')}")
    print(f"   Executable: {agent.validate_file_type('malware.exe')}")
    
    # Test file hash calculation
    test_content = b"This is test file content"
    file_hash = agent.calculate_file_hash(test_content)
    print(f"\nüî¢ Test file hash: {file_hash[:16]}...")
    
    # Test attachment processing
    print(f"\nüìé Testing attachment processing:")
    test_attachment = {
        'filename': 'test_document.pdf',
        'content': test_content
    }
    test_email = {
        'sender_email': 'test@example.com',
        'sender_name': 'Test Sender',
        'subject': 'Test Email',
        'date': datetime.now().isoformat()
    }
    
    result = await agent.process_single_attachment(test_attachment, test_email)
    print(f"   Result: {result.status.value}")
    print(f"   Hash: {result.file_hash[:16] if result.file_hash else 'None'}...")
    print(f"   Confidence: {result.confidence}")
    
    # Get statistics
    stats = agent.get_processing_stats()
    print(f"\nüìä Processing statistics: {stats}")
    
    print("\nüìã Phase 1 implementation complete!")
    print("   ‚úÖ File type and size validation")
    print("   ‚úÖ SHA256 hashing")
    print("   ‚úÖ ClamAV antivirus integration")
    print("   ‚úÖ Processing pipeline foundation")
    print("   ‚úÖ Health monitoring")

if __name__ == "__main__":
    asyncio.run(test_asset_document_agent()) 